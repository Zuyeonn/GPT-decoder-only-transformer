{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66e1cc4c-2ea1-443c-8f2a-c428997ce05b",
   "metadata": {},
   "source": [
    "## 코드 개요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7540bf3-8e76-4444-aed8-6bd0c672dcf3",
   "metadata": {},
   "source": [
    "#### 논문: https://arxiv.org/pdf/2407.20775\n",
    "#### 깃허브: https://github.com/harryjdavies/HeartGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf48f00-eab0-436d-908f-ce15f3a43662",
   "metadata": {},
   "source": [
    "이 코드의 목적은 ECG(심전도) 데이터를 기반으로 트랜스포머 모델을 사전 학습(Pretraining)하는 것이다.\n",
    "특히 GPT 기반의 트랜스포머를 사용하여 다음 시점(time step)의 신호 값을 예측하는 방식으로 학습이 진행된다.\n",
    "\n",
    "**모델 구조**\n",
    "- GPT(Decoder-Only Transformer) 기반\n",
    "- 8개 어텐션 헤드 (n_head = 8)\n",
    "- 8개 트랜스포머 블록 (n_layer = 8)\n",
    "- 맥락 길이(context length) 500\n",
    "- 임베딩 차원 64 (n_embd = 64)\n",
    "- 드롭아웃(dropout) 0.2 적용\n",
    "\n",
    "**학습 데이터**\n",
    "- .mat 파일(ECG 신호 데이터)에서 불러옴\n",
    "- 모든 데이터는 0~100 범위로 정규화한 후 101개의 토큰으로 변환\n",
    "- 90%는 학습(train), 10%는 테스트(test) 데이터로 분할\n",
    "\n",
    "**손실 함수**\n",
    "\n",
    "- Cross Entropy Loss: 다음 시점의 신호 값을 예측하는 문제를 다중 분류 문제로 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132de68c-f442-4965-9dc2-c41b806e2590",
   "metadata": {},
   "source": [
    "## 필수 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453da5ab-cf29-4a06-807d-25ef49c67b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn  # 신경망 모델을 구성하는 모듈 (nn.Linear, nn.LayerNorm, nn.Dropout 등)\n",
    "from torch.nn import functional as F  # 활성화 함수(F.softmax), 손실 함수(F.cross_entropy) 등을 포함\n",
    "import scipy.io  # .mat 파일에서 데이터를 로드하기 위한 라이브러리\n",
    "import numpy as np\n",
    "import time  # 성능 측정을 위한 라이브러리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa324f6-6b53-449c-8b5e-480f9823287f",
   "metadata": {},
   "source": [
    "## 하이퍼파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1677acc0-3ec8-4b76-9ac2-9e61f79cd81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harry Davies 19_09_2024\n",
    "\n",
    "# The following code is adapted from a tutorial by Andrej Kapathy, available at https://github.com/karpathy/ng-video-lecture\n",
    "# The explaination behind this code and the model files can be found in the paper \"Interpretable Pre-Trained Transformers for Heart Time-Series Data\"\n",
    "# available at https://arxiv.org/abs/2407.20775\n",
    "\n",
    "eval_interval = 2000\n",
    "save_interval = 20000 #how often the model is checkpointed\n",
    "eval_iters = 200\n",
    "batch_size = 64 # sequences we process in parellel\n",
    "max_iters = 1000000\n",
    "block_size = 500 # this is context length\n",
    "learning_rate = 3e-04\n",
    "n_embd = 64 # 384 / 6 means every head is 64 dimensional\n",
    "n_head = 8\n",
    "n_layer = 8\n",
    "dropout = 0.2\n",
    "\n",
    "\n",
    "# GPU is necessary. Training of 8 head, 8 layer model and 500 context length was possible with 12GB VRAM\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23b1a79-d6d2-4d9e-9747-bf6dcb9c74e6",
   "metadata": {},
   "source": [
    "- 컨텍스트 길이(block_size = 500)\n",
    "→ 트랜스포머 모델이 500개 토큰의 신호를 보고 다음 값을 예측\n",
    "- 어텐션 헤드(n_head = 8) → 8개의 병렬 어텐션을 수행하여 특징을 학습\n",
    "- 드롭아웃(dropout = 0.2) → 과적합 방지를 위해 20%의 뉴런을 랜덤하게 비활성화\n",
    "- 최대 학습 횟수(max_iters = 1000000) → 100만 번의 업데이트 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0acba72-5e83-4d65-8f23-0067234c3370",
   "metadata": {},
   "source": [
    "## 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5eb740-06b8-46d8-8de9-74aaafea1e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_load = scipy.io.loadmat('D:/ecg_store_gpt_training') # .mat 파일에서 데이터 불러오기\n",
    "data_ecg = data_load['ecg_store']\n",
    "\n",
    "vocab_size = 101  # 신호 값을 0~100 범위의 정수형으로 변환 (101개 토큰)\n",
    "\n",
    "perm = np.random.permutation(data_ecg.shape[0])  # 데이터 랜덤으로 섞기\n",
    "data_ecg_rand = data_ecg[perm, :]\n",
    "\n",
    "data = torch.tensor(data_ecg_rand, dtype=torch.long)\n",
    "\n",
    "# 90% 학습 데이터, 10% 테스트 데이터로 분할\n",
    "x_thresh = int(0.9 * data_ecg.shape[0])\n",
    "train_data = data[:x_thresh, :]\n",
    "test_data = data[x_thresh:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b5f2e7-4d02-484d-a319-ebd230957aae",
   "metadata": {},
   "source": [
    "- ECG 데이터를 정수형 토큰으로 변환하여 트랜스포머에서 학습할 수 있도록 함 \n",
    "- 90%를 학습 데이터, 10%를 검증 데이터로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243658b8-7cf4-4ce4-bef7-8700a4675536",
   "metadata": {},
   "source": [
    "## 배치(batch) 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2659697f-ee37-40d3-a82a-1627ba40d12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data  if split == 'train' else test_data\n",
    "    # creates two random indices. One to pick the subject, and one to pick the position in the trace.\n",
    "    # traces for each subject were never less than 1000 samples. blocksize+ix2 can never be longer than the trace.\n",
    "    ix = torch.randint(data.shape[0], (batch_size,))\n",
    "    ix2 = torch.randint(500, (1,))\n",
    "    x = torch.stack([data[i,ix2:ix2+block_size] for i in ix])\n",
    "    y = torch.stack([data[i,ix2+1:ix2+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922e2f40-687b-4512-89c4-ecf723340e50",
   "metadata": {},
   "source": [
    "- 배치 데이터를 무작위로 선택하여 모델에 공급\n",
    "- x: 현재 시점의 데이터 (입력)\n",
    "- y: 다음 시점의 데이터 (타겟)\n",
    "- 모델이 현재 데이터를 보고 다음 시점 데이터를 예측하도록 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b32387e-cc11-4af3-98a9-be767d92208f",
   "metadata": {},
   "source": [
    "## 모델 손실 평가 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dca74e-b410-491f-a730-aa4d019956d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9464bae-df8c-429b-8947-a6358ad49fc2",
   "metadata": {},
   "source": [
    "- @torch.no_grad(): 그래디언트 계산을 비활성화하여 성능 최적화\n",
    "- 모델을 평가 모드(model.eval())로 설정하여 Dropout 등을 비활성화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472fd7ba-ba92-40c6-bccd-cd3fb064afa7",
   "metadata": {},
   "source": [
    "## 어텐션(Attention) 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f79142d-c2e2-4811-baf7-4d44928ab396",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones((block_size,block_size)))) #buffer means not updated by optimiser\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #start = time.time()\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        # compute attention scores (affinities)\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 # square root headsize # (B, T, C) @ (B, C, T) = B, T, T\n",
    "        # for every batch, we will now have a T by T matrix giving us the affinities of each token\n",
    "        wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n",
    "        # the tril signifies a decoder block, future tokens cannot communicate with the past\n",
    "        wei = F.softmax(wei, dim=-1)# all attention weights sum to 1 for updating a single token\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        #end = time.time()\n",
    "        #print(start-end)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c648aeb-2775-49dc-8d5d-eaaed0997211",
   "metadata": {},
   "source": [
    "어텐션 메커니즘 (Q, K, V 행렬 계산) , 어텐션 가중치 계산 및 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c25904a-f964-4951-bf53-0606ce506d7d",
   "metadata": {},
   "source": [
    "## 다중 헤드 어텐션 (MultiHeadAttention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdfcfdb-97b9-42c9-97e0-e6427b2d0fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        # creating a list of head objects (turned into modules) resulting in a number of head modules\n",
    "        # then assigns the list of modules to self.heads - these run in parellel\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd) #projection generally matches sizes for adding in residual connection\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #concatenate the output of the different attention heads\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368edede-af16-4208-afe8-ea55010e1503",
   "metadata": {},
   "source": [
    "## 피드포워드 신경망 (FeedForward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3521f520-0614-41bb-b48a-814da9cb3226",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),  # 차원을 4배 증가\n",
    "            nn.ReLU(),  # 활성화 함수 ReLU\n",
    "            nn.Linear(4 * n_embd, n_embd), #multiplication performed in attention is all you need paper (다시 원래 차원(n_embd)으로 축소)\n",
    "            # expands and contracts back down to projection\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3722efa-09c6-44db-af42-cc8cc40d5a2d",
   "metadata": {},
   "source": [
    "이 클래스는 트랜스포머 모델의 \"피드포워드 신경망(Feed Forward Neural Network, FFN)\"을 구현한 부분\n",
    "- 입력 차원(n_embd) → 중간 차원(4 * n_embd) → 출력 차원(n_embd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f856276-1c0a-49cb-a696-92d548caa5c5",
   "metadata": {},
   "source": [
    "## 트랜스포머 블록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6925dec-2360-4e97-a20c-c5c07eb298a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        # communication\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        # computation\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        # layer norm\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601c7644-e3cf-4ea8-93e6-7a9104eb19d7",
   "metadata": {},
   "source": [
    "Multi-Head Self-Attention + Feed Forward Network + Layer Normalization 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84a4b25-2a5c-4760-ad1f-8a54bc610bef",
   "metadata": {},
   "source": [
    "## GPT 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff94b10d-574e-4e0c-8fe8-7028c4b2b8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create heart GPT class\n",
    "class HeartGPTModel(nn.Module):\n",
    "\n",
    "    def __init__(self):  \n",
    "        super().__init__()\n",
    "        # table needs to be vocab size by vocab size, to look up probability of next token given this token\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size,n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head = n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        #idx is batch, targets is time\n",
    "        tok_emb = self.token_embedding_table(idx) #(B, T, vocab_size) which is batch, time, channel\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # T, C (integers from 0 to T-1)\n",
    "        x = tok_emb + pos_emb # B, T, C\n",
    "        x = self.blocks(x) # B, T, C\n",
    "        x = self.ln_f(x) # B, T, C\n",
    "\n",
    "        logits = self.lm_head(x)\n",
    "        #channel is vocab size, so in this case 65\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx (context) to the last block_size tokens because positional embeddings only has up to block size\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cf7c7e-af13-4965-b6e2-d7f96c6bd30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HeartGPTModel()\n",
    "m = model.to(device)\n",
    "# random loss at this point would be -log(1/65)\n",
    "\n",
    "#AdamW\n",
    "optimizer  = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# counter the number of model parameters to be trained\n",
    "num_parameters = count_parameters(model)\n",
    "print(f\"The model has {num_parameters} trainable parameters.\")\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    \n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    if iter % save_interval == 0:\n",
    "        #model_path for checkpointing\n",
    "        model_path = f\"D:/ECGPT_pretrained_{n_embd}_{n_head}_{n_layer}_{block_size}_{max_iters}_{iter}.pth\"\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "    #get batch\n",
    "    x_batch, y_batch = get_batch('train')\n",
    "\n",
    "    # loss evaluation\n",
    "    logits, loss = m(x_batch, y_batch)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
